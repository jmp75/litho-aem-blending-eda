{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canberra lithologies case study\n",
    "\n",
    "Motivated by learning that the ACT is interested in managed aquifer recharge for watering some green spaces.\n",
    "\n",
    "This notebook does not look at AEM data although sitting under a repository suggesting so. \n",
    "\n",
    "## Downloading the data \n",
    "\n",
    "Not throughly documented.\n",
    "\n",
    "Data was downloaded from the usual places, NGIS and Elvis. NGIS when using the Murrumbidgee catchment was actually not including the bores in the ACT, so needed to download the ACT ones also, and this present notebook will do the merging of the lithology logs. Spatial locations were merged manually, and subsetted, in QGIS\n",
    "\n",
    "Some of the data output by this present notebook fed into a [lithology log viewer](https://github.com/csiro-hydrogeology/lithology-viewer) that can be run as a dashboard on Binder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T01:54:38.357642Z",
     "start_time": "2018-02-27T01:54:36.460827Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set to True for co-dev of ela from this use case:\n",
    "ela_from_source = False\n",
    "ela_from_source = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ela_from_source:\n",
    "    if ('ELA_SRC' in os.environ):\n",
    "        root_src_dir = os.environ['ELA_SRC']\n",
    "    elif sys.platform == 'win32':\n",
    "        root_src_dir = r'C:\\src\\github_jm\\pyela'\n",
    "    else:\n",
    "        username = os.environ['USER']\n",
    "        root_src_dir = os.path.join('/home', username, 'src/ela/pyela')\n",
    "    pkg_src_dir = root_src_dir\n",
    "    sys.path.insert(0, pkg_src_dir)\n",
    "\n",
    "from ela.textproc import *\n",
    "from ela.utils import *\n",
    "from ela.classification import *\n",
    "from ela.visual import *\n",
    "from ela.spatial import SliceOperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "There are two main sets of information we need: the borehole lithology logs, and the spatial information in the surface elevation (DEM) and geolocation of a subset of bores around Bungendore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to explicitly set `data_path` to the location where you put the folder(s) e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/home/myusername/data' # On Linux, if you now have the folder /home/myusername/data/Bungendore\n",
    "#data_path = r'C:\\data\\Lithology'  # windows, if you have C:\\data\\Lithology\\Bungendore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise a fallback for the pyela developer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path is None:\n",
    "    if ('ELA_DATA' in os.environ):\n",
    "        data_path = os.environ['ELA_DATA']\n",
    "    elif sys.platform == 'win32':\n",
    "        data_path = r'C:\\data\\Lithology'\n",
    "    else:\n",
    "        username = os.environ['USER']\n",
    "        data_path = os.path.join('/home', username, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_datadir = os.path.join(data_path, 'Canberra')\n",
    "cbr_datadir_out = os.path.join(cbr_datadir, 'out')\n",
    "ngis_datadir = os.path.join(data_path, 'NGIS')\n",
    "act_shp_datadir = os.path.join(ngis_datadir, 'shp_ACT')\n",
    "bidgee_shp_datadir = os.path.join(ngis_datadir, 'shp_murrumbidgee_river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_outputs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = rasterio.open(os.path.join(cbr_datadir,'CLIP.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "show(dem,title='Canberra', cmap='terrain',  ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations_raw = gpd.read_file(os.path.join(cbr_datadir, 'Bores/act_bores.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations_raw.crs, dem.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DEM raster and the bore location shapefile do not use the same projection (coordinate reference system) so we reproject one of them. We choose the raster's UTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations = bore_locations_raw.to_crs(dem.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this location we actually had to download two data sets from the NGIS: the data for the murrumbidgee catchment does not include much of the ones also inside the ACT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_logs_act = pd.read_csv(os.path.join(act_shp_datadir, 'NGIS_LithologyLog.csv'))\n",
    "lithology_logs_bidgee = pd.read_csv(os.path.join(bidgee_shp_datadir, 'NGIS_LithologyLog.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lithology_logs_act), len(lithology_logs_bidgee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_logs = pd.concat([lithology_logs_act, lithology_logs_bidgee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "show(dem,title='Canberra', cmap='terrain',  ax=ax)\n",
    "bore_locations.plot(ax=ax, facecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a copy of the logs merged, so that we can fall back on to the original one if we mess things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T00:50:36.439052Z",
     "start_time": "2018-02-06T00:50:36.434305Z"
    }
   },
   "outputs": [],
   "source": [
    "df = lithology_logs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are probably the defaults from the ela package imports, but to be explicit:\n",
    "DEPTH_FROM_COL = 'FromDepth'\n",
    "DEPTH_TO_COL = 'ToDepth'\n",
    "\n",
    "TOP_ELEV_COL = 'TopElev'\n",
    "BOTTOM_ELEV_COL = 'BottomElev'\n",
    "\n",
    "LITHO_DESC_COL = 'Description'\n",
    "HYDRO_CODE_COL = 'HydroCode'\n",
    "\n",
    "HYDRO_ID_COL = 'HydroID'\n",
    "BORE_ID_COL = 'BoreID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that there are locations registered for which there is actually no lithology logs recorded. We want to keep boreholes that have at least one row in the lithology logs.\n",
    "\n",
    "TODO: this should be a feature in the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = set(df[BORE_ID_COL].values)\n",
    "geolog_ids = set(bore_locations[HYDRO_ID_COL].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ids), len(geolog_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = df_ids.intersection(geolog_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bore_locations[HYDRO_ID_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations = bore_locations[s.isin(keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually we do have indeed a few less bores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "show(dem,title='Canberra', cmap='terrain',  ax=ax)\n",
    "bore_locations.plot(ax=ax, facecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset further to a location of interest\n",
    "\n",
    "Here, we devised how we could reduce the area further for the purpose of a case study as small as possible for submission to a gallery (pyvista). However we ended up with not enough classified bores and missing data everywhere. Selecting data sets size  with enough data is needed. Tricky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T00:50:58.799366Z",
     "start_time": "2018-02-06T00:50:55.503766Z"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "# max/min bounds\n",
    "shp_bbox = get_bbox(bore_locations)\n",
    "shp_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_bbox = dem.bounds\n",
    "raster_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min,x_max,y_min,y_max  = intersecting_bounds([shp_bbox, raster_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = cookie_cut_gpd(bore_locations, x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "show(dem,title='Canberra', cmap='terrain',  ax=ax)\n",
    "trial.plot(ax=ax, facecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried to use only a further subset but there is not enough data to do the interpolation (too many \"none\" descriptions)\n",
    "# Parking this for now.\n",
    "\n",
    "# bore_locations = trial\n",
    "# shp_bbox = get_bbox(trial)\n",
    "\n",
    "# x_min = shp_bbox[0]\n",
    "# x_max = shp_bbox[2]\n",
    "# y_min = shp_bbox[1]\n",
    "# y_max = shp_bbox[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the geolocation from the shapefile and lithology records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geopandas data frame has a column geometry listing `POINT` objects. 'ela' includes  `get_coords_from_gpd_shape` to extrace the coordinates to a simpler structure. 'ela' has predefined column names (e.g. EASTING_COL) defined for easting/northing information, that we can use to name our coordinate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoloc_df(bore_locations, additional_columns):\n",
    "    geoloc = get_coords_from_gpd_shape(bore_locations, colname='geometry', out_colnames=[EASTING_COL, NORTHING_COL])\n",
    "    for cn in additional_columns:\n",
    "        geoloc[cn] = bore_locations[cn].values #important to remove indexing otherwise conterintuitive behavior (NaN)\n",
    "    return geoloc\n",
    "        \n",
    "geoloc = get_geoloc_df(bore_locations, ['Latitude', 'Longitude', HYDRO_ID_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be reused in experimental notebooks:\n",
    "geoloc_filename = os.path.join(cbr_datadir_out,'geoloc.pkl')\n",
    "if not os.path.exists(geoloc_filename):\n",
    "    geoloc.to_pickle(geoloc_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoloc.to_pickle(geoloc_filename)\n",
    "# geoloc.to_csv(os.path.join(cbr_datadir_out,'geoloc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc[HYDRO_ID_COL].dtype, df[BORE_ID_COL].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, geoloc, how='inner', left_on=BORE_ID_COL, right_on=HYDRO_ID_COL, sort=False, copy=True, indicator=False, validate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round up 'depth to' and 'depth from' columns\n",
    "\n",
    "We round the depth related columns to the upper integer value and drop the entries where the resulting depths have degenerated to 0. `ela` has a class `DepthsRounding` to facilitate this operations on lithology records with varying column names.\n",
    "\n",
    "We first clean up height/depths columns to make sure they are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: function in the package\n",
    "def as_numeric(x):\n",
    "    if isinstance(x, float):\n",
    "        return x\n",
    "    if x == 'None':\n",
    "        return np.nan\n",
    "    elif x is None:\n",
    "        return np.nan\n",
    "    elif isinstance(x, str):\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[DEPTH_FROM_COL] = df[DEPTH_FROM_COL].apply(as_numeric)\n",
    "df[DEPTH_TO_COL] = df[DEPTH_TO_COL].apply(as_numeric)\n",
    "df[TOP_ELEV_COL] = df[TOP_ELEV_COL].apply(as_numeric)\n",
    "df[BOTTOM_ELEV_COL] = df[BOTTOM_ELEV_COL].apply(as_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DepthsRounding(DEPTH_FROM_COL, DEPTH_TO_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Before rounding heights we have \" + str(len(df)) + \" records\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dr.round_to_metre_depths(df, np.round, True)\n",
    "\"After removing thin sliced entries of less than a metre, we are left with \" + str(len(df)) + \" records left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the descriptive lithology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = df[LITHO_DESC_COL]\n",
    "descs = descs.reset_index()\n",
    "descs = descs[LITHO_DESC_COL]\n",
    "descs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description column as read seems to be objects. Other columns seem to be objects when they should be numeric. We define two functions to clean these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_desc(x):\n",
    "    if isinstance(x, float):\n",
    "        return u''\n",
    "    elif x is None:\n",
    "        return u''\n",
    "    else:\n",
    "        # python2 return unicode(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [clean_desc(x) for x in descs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from striplog import Lexicon\n",
    "lex = Lexicon.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean_lithology_descriptions(y, lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a flat list of all the \"tokens\" but remove stop words ('s', 'the' and the like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = v_lower(y)\n",
    "vt = v_word_tokenize(y)\n",
    "flat = np.concatenate(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "exclude = stoplist + ['.',',',';',':','(',')','-']\n",
    "flat = [word for word in flat if word not in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common= token_freq(flat, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(df_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining lithology classes and finding primary/secondary lithologies\n",
    "\n",
    "From the list of most common tokens, we may want to define lithology classes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[LITHO_DESC_COL] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies = [        'shale', 'clay','granite','soil','sand', 'porphyry','siltstone', 'dacite', 'gravel', 'limestone']\n",
    "# Prep for visualisation\n",
    "lithology_color_names = ['lightslategrey', 'olive', 'dimgray', 'chocolate',  'gold', 'tomato', 'teal', 'darkgrey', 'lavender', 'yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more classes for display of raw logs\n",
    "lithologies = ['shale', 'clay','granite','soil','sand', 'porphyry','siltstone', 'dacite', 'rhyodacite', 'gravel', 'limestone', 'sandstone', 'slate', 'mudstone', 'rock', 'ignimbrite', 'tuff']\n",
    "# Prep for visualisation\n",
    "lithology_color_names = [\n",
    "    'lightslategrey', # Shale\n",
    "    'olive', # clay\n",
    "    'dimgray', # granite\n",
    "    'chocolate',  # soil\n",
    "    'gold', # sand\n",
    "    'tomato', # porphyry\n",
    "    'teal', # siltstone\n",
    "    'darkgrey', # dacite\n",
    "    'whitesmoke', # rhyodacite\n",
    "    'powderblue', # gravel \n",
    "    'yellow', #limestone\n",
    "    'papayawhip', #sandstone\n",
    "    'dimgray', #slate\n",
    "    'darkred', #mudstone\n",
    "    'grey', #rock\n",
    "    'khaki', #ignimbrite\n",
    "    'lemonchiffon' #tuff\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to capture any of these we devise a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lithologies_numclasses = create_numeric_classes(lithologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies_dict = dict([(x,x) for x in lithologies])\n",
    "# Plurals do occur\n",
    "lithologies_dict['clays'] = 'clay'\n",
    "lithologies_dict['sands'] = 'sand'\n",
    "lithologies_dict['shales'] = 'shale'\n",
    "\n",
    "\n",
    "# lithologies_dict['dacite'] = 'granite'\n",
    "# lithologies_dict['sandstone'] = 'granite'\n",
    "# lithologies_dict['slate'] = 'granite'\n",
    "# lithologies_dict['rock'] = 'granite'\n",
    "# lithologies_dict['ryodacite'] = 'granite'\n",
    "# lithologies_dict['mudstone'] = 'sand' # ??\n",
    "lithologies_dict['topsoil'] = 'soil' # ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_litho_markers_re = r'shale|clay|granit|soil|sand|porphy|silt|gravel|dacit|slat|rock|stone|slate|brite|tuff'\n",
    "regex = re.compile(any_litho_markers_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies_adjective_dict = {\n",
    "    'sandy' :  'sand',\n",
    "    'clayey' :  'clay',\n",
    "    'clayish' :  'clay',\n",
    "    'shaley' :  'shale',\n",
    "    'silty' :  'silt',\n",
    "    'pebbly' :  'pebble',\n",
    "    'gravelly' :  'gravel',\n",
    "    'porphyritic': 'porphyry'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tokens = v_word_tokenize(y)\n",
    "litho_terms_detected = v_find_litho_markers(v_tokens, regex=regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we detect these lithology markers in each bore log entries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mark = [x for x in litho_terms_detected if len(x) == 0 ]\n",
    "at_least_one_mark = [x for x in litho_terms_detected if len(x) >= 1]\n",
    "at_least_two_mark = [x for x in litho_terms_detected if len(x) >= 2]\n",
    "print('There are %s entries with no marker, %s entries with at least one, %s with at least two'%(len(zero_mark),len(at_least_one_mark),len(at_least_two_mark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: probably need to think of precanned facilities in ela to assess the detection rate in such EDA. Maybe wordcloud not such a bad idea too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs_zero_mark = [y[i] for i in range(len(litho_terms_detected)) if len(litho_terms_detected[i]) == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(descs_zero_mark,20)\n",
    "# descs_zero_mark[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flat_list_tokens(descs_zero_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(s, title = 'Unclassified via regexp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_litho = v_find_primary_lithology(litho_terms_detected, lithologies_dict)\n",
    "secondary_litho = v_find_secondary_lithology(litho_terms_detected, primary_litho, lithologies_adjective_dict, lithologies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[PRIMARY_LITHO_COL]=primary_litho\n",
    "df[SECONDARY_LITHO_COL]=secondary_litho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[PRIMARY_LITHO_NUM_COL] = v_to_litho_class_num(primary_litho, my_lithologies_numclasses)\n",
    "df[SECONDARY_LITHO_NUM_COL] = v_to_litho_class_num(secondary_litho, my_lithologies_numclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting depth below ground to Australian Height Datum elevation\n",
    "\n",
    "While the bore entries have columns for AHD elevations, many appear to be missing data. Since we have a DEM of the region we can correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = HeightDatumConverter(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cd.add_height(df, \n",
    "        depth_from_col=DEPTH_FROM_COL, depth_to_col=DEPTH_TO_COL, \n",
    "        depth_from_ahd_col=DEPTH_FROM_AHD_COL, depth_to_ahd_col=DEPTH_TO_AHD_COL, \n",
    "        easting_col=EASTING_COL, northing_col=NORTHING_COL, drop_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be reused in experimental notebooks:\n",
    "classified_logs_filename = os.path.join(cbr_datadir_out,'classified_logs.pkl')\n",
    "if write_outputs or not os.path.exists(classified_logs_filename):\n",
    "    df.to_pickle(classified_logs_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(classified_logs_filename)\n",
    "# df.to_csv(os.path.join(cbr_datadir_out,'classified_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_logs_filename = os.path.join(cbr_datadir_out,'classified_logs.csv')\n",
    "df_subset = df[[HYDRO_ID_COL, BORE_ID_COL, DEPTH_FROM_COL, DEPTH_TO_COL, LITHO_DESC_COL, 'Lithology_1', 'MajorLithCode']]\n",
    "# df_subset.to_csv(classified_logs_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interpolate over a regular grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res = 200\n",
    "m = create_meshgrid_cartesian(x_min, x_max, y_min, y_max, grid_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T00:50:58.799366Z",
     "start_time": "2018-02-06T00:50:55.503766Z"
    }
   },
   "outputs": [],
   "source": [
    "dem_array = surface_array(dem, x_min, y_min, x_max, y_max, grid_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T00:50:58.799366Z",
     "start_time": "2018-02-06T00:50:55.503766Z"
    }
   },
   "outputs": [],
   "source": [
    "dem_array[dem_array <= 0.0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T00:50:58.799366Z",
     "start_time": "2018-02-06T00:50:55.503766Z"
    }
   },
   "outputs": [],
   "source": [
    "dem_array_data = {'bounds': (x_min, x_max, y_min, y_max), 'grid_res': grid_res, 'mesh_xy': m, 'dem_array': dem_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fp = os.path.join(cbr_datadir_out, 'dem_array_data.pkl')\n",
    "if write_outputs or not os.path.exists(fp):\n",
    "    with open(fp, 'wb') as handle:\n",
    "        pickle.dump(dem_array_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define min and max heights on the Z axis for which we interoplate. We use the KNN algorithm with 10 neighbours. We should use a domain such that there are enough points for each height. Let's find visually heights with at least 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DepthCoverage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, cc = dc.get_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dc.get_range(11)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours=10\n",
    "ahd_min=int(r[0])\n",
    "ahd_max=int(r[1])\n",
    "\n",
    "z_ahd_coords = np.arange(ahd_min,ahd_max,1)\n",
    "dim_x,dim_y = m[0].shape\n",
    "dim_z = len(z_ahd_coords)\n",
    "dims = (dim_x,dim_y,dim_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_3d_array=np.empty(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi = GridInterpolation(easting_col=EASTING_COL, northing_col=NORTHING_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi.get_lithology_observations_for_depth(df, ahd_max, 'Depth From (AHD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi.interpolate_volume(lithology_3d_array, df, PRIMARY_LITHO_NUM_COL, z_ahd_coords, n_neighbours, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burn DEM into grid\n",
    "z_index_for_ahd = z_index_for_ahd_functor(b=-ahd_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_array.shape, m[0].shape, lithology_3d_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_volume(lithology_3d_array, dem_array, z_index_for_ahd, below=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be reused in experimental notebooks:\n",
    "interp_litho_filename = os.path.join(cbr_datadir_out,'3d_primary_litho.pkl')\n",
    "if write_outputs or not os.path.exists(interp_litho_filename):\n",
    "    with open(interp_litho_filename, 'wb') as handle:\n",
    "        pickle.dump(lithology_3d_array, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 ELA",
   "language": "python",
   "name": "ela"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
